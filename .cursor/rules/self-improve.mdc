---
description: Pattern-based rule improvement with consent-gated proposals
alwaysApply: true
lastReviewed: 2025-10-16
healthScore:
  content: green # Adapted from taskmaster with consent modifications
  usability: green # Clear observation vs action separation
  maintenance: green # New rule, aligned with repository standards
---

## Rule Improvement Triggers

- New code patterns not covered by existing rules
- Repeated similar implementations across files (3+ instances)
- Common error patterns that could be prevented
- New libraries or tools being used consistently (3+ modules)
- Emerging best practices in the codebase
- Recurring feedback in clarifications or code reviews
- **User corrections to process order** (e.g., "document first", "plan before implementing")
- **Rule gaps observed during active investigations** (see Special Cases below)

## Analysis Process

- Compare new code with existing rules
- Identify patterns that should be standardized
- Look for references to external documentation
- Check for consistent error handling patterns
- Monitor test patterns and coverage

## Observation (always-on, passive)

- During normal work, silently note patterns meeting trigger thresholds
- No tool calls, logs, or file writes during observation
- Session-scoped state only; observations reset at session boundary
- Queue proposals until natural checkpoints (never interrupt mid-task)

## Proposals (consent-gated, checkpointed)

Surface proposals only at natural boundaries:

- After Green in TDD cycle (tests passing)
- After PR created (via `.cursor/scripts/pr-create.sh`)
- After task marked complete (via `todo_write` with status: completed)
- When user asks "anything else?" or similar wrap-up phrase

Proposal format:

- Pattern: <one-line description>
- Evidence: <file paths or commit refs>
- Proposed change: <add/modify/deprecate rule X>
- Impact: <who benefits, what improves>

User responses:

- "Yes"/"Proceed"/"Go ahead" → create rule update with evidence
- "No"/"Skip"/"Not now" → suppress for session; do not re-ask
- Silence → drop and continue

## Rule Updates

**Add New Rules When:**

- A new technology/pattern is used in 3+ files
- Common bugs could be prevented by a rule
- Code reviews repeatedly mention the same feedback
- New security or performance patterns emerge

**Modify Existing Rules When:**

- Better examples exist in the codebase
- Additional edge cases are discovered
- Related rules have been updated
- Implementation details have changed

## Example Pattern Recognition

```typescript
// If you see repeated patterns like:
const data = await prisma.user.findMany({
  select: { id: true, email: true },
  where: { status: "ACTIVE" },
});

// Consider proposing update to relevant rule:
// - Standard select fields
// - Common where conditions
// - Performance optimization patterns
```

## Rule Quality Checks

- Rules should be actionable and specific
- Examples should come from actual code
- References should be up to date
- Patterns should be consistently enforced

## Continuous Improvement

- Observe code patterns during normal work
- Track common development questions
- Note patterns after major refactors
- Identify links to relevant documentation
- Cross-reference related rules

### Process-Order Patterns

When user corrects the order of operations (e.g., "should document that first", "plan before implementing"):

1. **Immediate response**: Acknowledge and document the protocol/plan immediately
2. **Pattern recognition**: This signals missing guidance about process order
3. **Meta-observation**: Add to investigation findings if investigating process/rules
4. **Rule update consideration**: If pattern recurs (2+ instances), queue rule improvement

**Common process-order corrections**:

- "Document the test protocol first" → Document → then choose execution mode
- "Plan before implementing" → Create spec/plan → then implement
- "Define acceptance criteria first" → Write criteria → then build
- "Create test first" → Write failing test → then implement (TDD)

## Rule Deprecation

When patterns become outdated:

- Mark outdated patterns as deprecated in rules
- Remove rules that no longer apply
- Update references to deprecated rules
- Document migration paths for old patterns

## Documentation Updates

- Keep examples synchronized with code
- Update references to external docs
- Maintain links between related rules
- Document breaking changes

## Special Case: Rule Investigations

When actively investigating rule effectiveness or enforcement:

### During Active Investigations

**Observed rule gaps are first-class investigation data** (not optional enhancements):

1. **Notice gap → Document immediately**

   - Unclear guidance encountered? → Add to findings.md in real-time
   - Process confusion observed? → Note as investigation evidence
   - Rule conflict discovered? → Document the conflict immediately

2. **Don't wait for user prompts**

   - Bad: Wait for user to ask "did you notice X?"
   - Good: Flag gaps as they're observed during work
   - Proactive documentation, not reactive

3. **Treat as investigation evidence**

   - If investigating completion policies → completion confusion is data
   - If investigating enforcement patterns → enforcement gaps are data
   - If investigating rule quality → rule issues are data

4. **Queue improvements as investigation proceeds**
   - Add observed gaps to investigation scope
   - Include in findings/outcomes
   - Create improvement tasks from observations

### Meta-Consistency Requirement

**Apply findings to your own process**:

- Investigating gates? → Enforce gates on self during investigation
- Investigating completion? → Validate own completion rigorously
- Investigating scripts? → Use scripts consistently during investigation

**Why**: Violating patterns under investigation undermines credibility and misses valuable validation data

### Example: Rules-Enforcement Investigation

**What Happened** (incorrect):

- Investigated why project-lifecycle completion is unclear
- Noticed confusion during work
- Didn't flag gap until user prompted
- Result: Missed opportunity to capture real-time evidence

**What Should Happen** (correct):

- Notice confusion about completion states
- Immediately add to investigation scope: "Discovered gap in project-lifecycle.mdc"
- Document as evidence in findings.md
- Create improvement task from observation
- Result: Investigation captures its own meta-findings

**Evidence**: 6 rule gaps discovered during rules-enforcement-investigation, but only flagged reactively (after user prompting) rather than proactively as investigation data

### Checkpointing Rule Gap Observations

**During Investigation**:

- Real-time documentation in findings.md
- Add to investigation scope if relevant
- No separate consent needed (part of investigation work)

**After Investigation**:

- Follow normal consent-gated proposal workflow
- Present improvements at natural checkpoint
- Get approval before implementing

## Related

- See [assistant-behavior.mdc](./assistant-behavior.mdc) for consent-first behavior and checkpoints
- See [rule-creation.mdc](./rule-creation.mdc) for rule writing guidelines
- See [rule-maintenance.mdc](./rule-maintenance.mdc) for pattern-driven update workflow
- See [rule-quality.mdc](./rule-quality.mdc) for validation standards
