---
description: Pattern-based rule improvement with consent-gated proposals
alwaysApply: true
lastReviewed: 2025-10-23
healthScore:
  content: green # Includes investigation OUTPUT requirement, solution creation checklist
  usability: green # Clear observation vs action separation, explicit enforcement
  maintenance: green # Updated with Gap #17/17b enforcement mechanisms
---

## Rule Improvement Triggers

- New code patterns not covered by existing rules
- Repeated similar implementations across files (3+ instances)
- Common error patterns that could be prevented
- New libraries or tools being used consistently (3+ modules)
- Emerging best practices in the codebase
- Recurring feedback in clarifications or code reviews
- **User corrections to process order** (e.g., "document first", "plan before implementing")
- **Rule gaps observed during active investigations** (see Special Cases below)
- **Guidance exists in project docs but not in .cursor/rules/** (see Rule Creation below)

## Analysis Process

- Compare new code with existing rules
- Identify patterns that should be standardized
- Look for references to external documentation
- Check for consistent error handling patterns
- Monitor test patterns and coverage

## Observation (always-on, passive)

- During normal work, silently note patterns meeting trigger thresholds
- No tool calls, logs, or file writes during observation
- Session-scoped state only; observations reset at session boundary
- Queue proposals until natural checkpoints (never interrupt mid-task)

## Proposals (consent-gated, checkpointed)

Surface proposals only at natural boundaries:

- After Green in TDD cycle (tests passing)
- After PR created (via `.cursor/scripts/pr-create.sh`)
- After task marked complete (via `todo_write` with status: completed)
- When user asks "anything else?" or similar wrap-up phrase

Proposal format:

- Pattern: <one-line description>
- Evidence: <file paths or commit refs>
- Proposed change: <add/modify/deprecate rule X>
- Impact: <who benefits, what improves>

User responses:

- "Yes"/"Proceed"/"Go ahead" → create rule update with evidence
- "No"/"Skip"/"Not now" → suppress for session; do not re-ask
- Silence → drop and continue

## Rule Creation (Immediate Action Required)

**When discovering guidance that should be a rule**:

### Trigger: Useful Guidance Not in .cursor/rules/

**If you discover**:

- Guidance in project documentation (`docs/projects/`) that should apply during work
- Structure standards that need enforcement
- Patterns documented but not in rule files
- References to "follow X standard" where X is only in a separate project

**Then**:

1. **Flag immediately**: "This guidance should be a rule file"
2. **Check**: Does rule file exist in `.cursor/rules/`?
3. **If NO**: Propose creating it NOW (don't wait for checkpoint)
4. **If YES**: Link to existing rule instead

**Example (Gap #11)**:

- Discovered: `investigation-docs-structure/structure-standard.md` has clear guidance
- Checked: NO `.cursor/rules/investigation-structure.mdc` exists
- **Should have**: Proposed "Should I create investigation-structure.mdc?"
- **Actually did**: Violated structure without proposing rule
- **User caught**: "Seems like we need to create the rule file"

**Lesson**: Distinguish "missing rule" from "violated rule" - create before enforcing

### Decision Framework

```
Discover guidance → Check .cursor/rules/
├─ Rule exists → Link to it, follow it
└─ Rule missing → PROPOSE creating it immediately
    ├─ Extract key guidance
    ├─ Set appropriate globs/triggers
    ├─ Make actionable (checklists, decision trees)
    └─ Get user approval before creating
```

## Rule Updates

**Add New Rules When:**

- A new technology/pattern is used in 3+ files
- Common bugs could be prevented by a rule
- Code reviews repeatedly mention the same feedback
- New security or performance patterns emerge
- **Guidance exists in project docs but not in rules/** (create rule to make it accessible)

**Modify Existing Rules When:**

- Better examples exist in the codebase
- Additional edge cases are discovered
- Related rules have been updated
- Implementation details have changed

## Example Pattern Recognition

```typescript
// If you see repeated patterns like:
const data = await prisma.user.findMany({
  select: { id: true, email: true },
  where: { status: "ACTIVE" },
});

// Consider proposing update to relevant rule:
// - Standard select fields
// - Common where conditions
// - Performance optimization patterns
```

## Rule Quality Checks

- Rules should be actionable and specific
- Examples should come from actual code
- References should be up to date
- Patterns should be consistently enforced

## Continuous Improvement

- Observe code patterns during normal work
- Track common development questions
- Note patterns after major refactors
- Identify links to relevant documentation
- Cross-reference related rules

### Process-Order Patterns

When user corrects the order of operations (e.g., "should document that first", "plan before implementing"):

1. **Immediate response**: Acknowledge and document the protocol/plan immediately
2. **Pattern recognition**: This signals missing guidance about process order
3. **Meta-observation**: Add to investigation findings if investigating process/rules
4. **Rule update consideration**: If pattern recurs (2+ instances), queue rule improvement

**Common process-order corrections**:

- "Document the test protocol first" → Document → then choose execution mode
- "Plan before implementing" → Create spec/plan → then implement
- "Define acceptance criteria first" → Write criteria → then build
- "Create test first" → Write failing test → then implement (TDD)

## Rule Deprecation

When patterns become outdated:

- Mark outdated patterns as deprecated in rules
- Remove rules that no longer apply
- Update references to deprecated rules
- Document migration paths for old patterns

## Documentation Updates

- Keep examples synchronized with code
- Update references to external docs
- Maintain links between related rules
- Document breaking changes

## Special Case: Rule Investigations

When actively investigating rule effectiveness or enforcement:

### During Active Investigations

**Observed rule gaps are first-class investigation data** (not optional enhancements):

1. **Notice gap → Document immediately**

   - Unclear guidance encountered? → Add to findings.md in real-time
   - Process confusion observed? → Note as investigation evidence
   - Rule conflict discovered? → Document the conflict immediately

2. **Don't wait for user prompts**

   - Bad: Wait for user to ask "did you notice X?"
   - Good: Flag gaps as they're observed during work
   - Proactive documentation, not reactive

3. **OUTPUT investigation finding categorization** (Required)

   When observing any issue during investigation work, OUTPUT:

   ```
   Observed: [brief description]
   Category: [routing|execution|workflow|other]
   Project: [checked ACTIVE-MONITORING.md → project-name]
   Document in: [findings file path]
   ```

   **Rationale**: 
   - Explicit OUTPUT requirements achieve 100% visibility (validated: H2 findings)
   - Forces ACTIVE-MONITORING.md check before documentation
   - Prevents project scope confusion (routing vs execution vs workflow)
   - See: `docs/projects/ACTIVE-MONITORING.md` for project scopes and decision tree

   **Then** proceed to document in correct location.

4. **Treat as investigation evidence**

   - If investigating completion policies → completion confusion is data
   - If investigating enforcement patterns → enforcement gaps are data
   - If investigating rule quality → rule issues are data

5. **Queue improvements as investigation proceeds**
   - Add observed gaps to investigation scope
   - Include in findings/outcomes
   - Create improvement tasks from observations

### Meta-Consistency Requirement

**Apply findings to your own process**:

- Investigating gates? → Enforce gates on self during investigation
- Investigating completion? → Validate own completion rigorously
- Investigating scripts? → Use scripts consistently during investigation

**Why**: Violating patterns under investigation undermines credibility and misses valuable validation data

### Example: Rules-Enforcement Investigation

**What Happened** (incorrect):

- Investigated why project-lifecycle completion is unclear
- Noticed confusion during work
- Didn't flag gap until user prompted
- Result: Missed opportunity to capture real-time evidence

**What Should Happen** (correct):

- Notice confusion about completion states
- Immediately add to investigation scope: "Discovered gap in project-lifecycle.mdc"
- Document as evidence in findings.md
- Create improvement task from observation
- Result: Investigation captures its own meta-findings

**Evidence**: 6 rule gaps discovered during rules-enforcement-investigation, but only flagged reactively (after user prompting) rather than proactively as investigation data

### Solution Creation Checklist

**When proposing any new tool, guidance, or mechanism** (Required):

Before considering solution complete, specify:

1. **Enforcement**: How will this be used/triggered?
   - OUTPUT requirement? Pre-send gate item? AlwaysApply? Blocking gate?
   - Example: "ACTIVE-MONITORING.md will be checked via OUTPUT requirement in self-improve.mdc"

2. **Trigger**: When does this apply?
   - Project context? File pattern? Phase? User action?
   - Example: "Applies during investigation work when observing issues"

3. **Validation**: How to verify it's working?
   - Measurement approach? Success criteria?
   - Example: "100% of findings show OUTPUT with ACTIVE-MONITORING.md check"

**Anti-pattern**: Creating tools/guidance without enforcement specification

**Evidence**: Gap #17b (created ACTIVE-MONITORING.md without enforcement) validates this requirement

### Checkpointing Rule Gap Observations

**During Investigation**:

- Real-time documentation in findings.md
- Add to investigation scope if relevant
- No separate consent needed (part of investigation work)

**After Investigation**:

- Follow normal consent-gated proposal workflow
- Present improvements at natural checkpoint
- Get approval before implementing

## Related

- See [assistant-behavior.mdc](./assistant-behavior.mdc) for consent-first behavior and checkpoints
- See [rule-creation.mdc](./rule-creation.mdc) for rule writing guidelines
- See [rule-maintenance.mdc](./rule-maintenance.mdc) for pattern-driven update workflow
- See [rule-quality.mdc](./rule-quality.mdc) for validation standards
